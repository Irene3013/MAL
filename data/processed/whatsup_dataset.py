# data/processed/whatsup_dataset.py
from pathlib import Path
import random
import json
import os
import torch
from torch.utils.data import Dataset, DataLoader
from utils.data_helpers import whastup_dual_encoder_collate
import pytorch_lightning as pl
from PIL import Image
from utils.model_helpers import create_qwen_message, create_MC_qwen_message
from qwen_vl_utils import process_vision_info

# -----------------------------
# What's Up Dataset
# -----------------------------
class WhatsUpDataset(Dataset):
    """
    What's Up Dataset
    """
    def __init__(self, dataset_name="images", data_path="data", model=None, config=None):

        # Validations
        self.base_path = Path(data_path) / "raw" / "whatsup" #relative path
        assert self.base_path.exists(), f"Root directory '{self.base_path}' does not exist."   
        assert dataset_name in ['images', 'clevr'], f"Unsupported subset: '{dataset_name}'. Must be one of ['images', 'clevr']."
        
        # Get train/dev/test
        self.dataset_name = dataset_name
        self.model = model

        # Load dataset
        self.data_path = self.base_path / f"controlled_{dataset_name}_dataset.json"
        self.image_path = self.base_path / f"controlled_{dataset_name}"
        self.dataset = self._load_json()

        # Input processing
        self.transform = config["transform"]
        self.tokenizer = config["tokenizer"]
        self.processor = config["processor"]
        self.params = config.get("params", {})
        self.score = "mc"

    def _load_json(self):
        with open(self.data_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _load_image(self, orig_path):
        img_path = self.image_path / orig_path.split("/")[-1]
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image not found: {img_path}")
        return Image.open(img_path).convert("RGB")

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        if self.model in ["clip", "siglip", "siglip2", "pecore"]:
            return self._dual_encoder_item(item)
        
        elif self.model == "qwen2":
            return self._qwen_item(item)
            
        else:
            raise NotImplementedError()
    
    # --- ITEM METHODS ---
    def _dual_encoder_item(self, item):
        """Prepare item for Dual Encoder models."""  
        return {
            "image": self._load_image(item["image_path"]),
            "caption_options": item["caption_options"],
            "correct_option": item["caption_options"][0], # The first option is the correct one
        }
    
    def _qwen_item(self, item):
        """Prepare item for Qwen2-VL model."""
        img_path = self.image_path / str(item["image_path"]).split("/")[-1]
        correct = item["caption_options"][0]
        options = item["caption_options"]
        random.shuffle(options)

        # messages = create_MC_qwen_message(img_path, options)
        # text = self.processor.apply_chat_template(
        #     messages, tokenize=False, add_generation_prompt=True
        # )
        # image_inputs, video_inputs = process_vision_info(messages)
        # inputs = self.processor(
        #     text=[text],
        #     images=image_inputs,
        #     videos=video_inputs,
        #     padding=True,
        #     return_tensors="pt",
        # )
        # # Expected response to be generated by Qwen (ej: "A" or "B")
        # label_opt = ['A', 'B', 'C', 'D']
        # expected_response = label_opt[options.index(correct)]

        inputs = []
        messages = create_qwen_message(img_path, options)
        for message in messages: #Process image - message in pairs
            text = self.processor.apply_chat_template(
                message, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(message)
            input = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs.append(input)

        # Expected response to be generated by Qwen (ej: "True" or "False" for each)
        expected_response = ['False', 'False', 'False', 'False']
        expected_response[options.index(correct)] = 'True'

        return {
             "input": inputs,
             "label": expected_response,
             "options": options,
             "correct": [correct]
        }

    @staticmethod
    def compute_accuracy(logits, labels, score="precision"): 
        # TODO acc depending on score
        if score == "pair-wise":
            0
        elif score == "set-wise":
            0
        else: 
            probs = torch.sigmoid(logits)
            return (probs.argmax(dim=1) == labels).float().mean() 
        
# -----------------------------
# COCO-spatial Dataset
# -----------------------------
class COCOSpatialDataset(Dataset):
    """
    COCO-spatial Dataset
    """
    def __init__(self, dataset_name="one", data_path="data", image_path="data", model=None, config=None):

        # Validations
        self.base_path = Path(data_path) / "raw" / "COCO_spatial" #relative path
        assert self.base_path.exists(), f"Root directory '{self.base_path}' does not exist."   
        assert dataset_name in ['one', 'two'], f"Unsupported subset: '{dataset_name}'. Must be one of ['one', 'two']."
        
        # Get train/dev/test
        self.dataset_name = dataset_name
        self.model = model

        # Load dataset
        self.data_path = self.base_path / f"coco_qa_{dataset_name}_obj.json"
        self.image_path = image_path
        self.dataset = self._load_json()

        # Input processing
        self.transform = config["transform"]
        self.tokenizer = config["tokenizer"]
        self.processor = config["processor"]
        self.params = config.get("params", {})
        self.score = "mc"

    def _load_json(self):
        with open(self.data_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _load_image(self, image):
        img_path = Path(self.image_path) / f"{str(image).zfill(12)}.jpg"
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image not found: {img_path}")
        return Image.open(img_path).convert("RGB")

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        if self.model in ["clip", "siglip", "siglip2", "pecore"]:
            return self._dual_encoder_item(item)
        
        elif self.model in ["qwen2"]:
            return self._qwen_item(item)
            
        else:
            raise NotImplementedError()
        

    # --- ITEM METHODS ---
    def _dual_encoder_item(self, item):
        """Prepare item for Dual Encoder models."""  
        return {
            "image": self._load_image(item[0]),
            "caption_options": [str(item[1]), str(item[2])],
            "correct_option": str(item[1]), # The first option is the correct one
        }
    
    def _qwen_item(self, item):
        """Prepare item for Qwen2-VL model."""
        img_path = Path(self.image_path) / f"{str(item[0]).zfill(12)}.jpg"
        correct = str(item[1])
        options = [str(item[1]), str(item[2])]
        random.shuffle(options)

        # messages = create_MC_qwen_message(img_path, options)
        # text = self.processor.apply_chat_template(
        #     messages, tokenize=False, add_generation_prompt=True
        # )
        # image_inputs, video_inputs = process_vision_info(messages)
        # inputs = self.processor(
        #     text=[text],
        #     images=image_inputs,
        #     videos=video_inputs,
        #     padding=True,
        #     return_tensors="pt",
        # )
        # # Expected response to be generated by Qwen (ej: "A" or "B")
        # label_opt = ['A', 'B']
        # expected_response = label_opt[options.index(correct)]
        
        inputs = []
        messages = create_qwen_message(img_path, options)
        for message in messages: #Process image - message in pairs
            text = self.processor.apply_chat_template(
                message, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(message)
            input = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs.append(input)

        # Expected response to be generated by Qwen (ej: "True" or "False" for each)
        expected_response = ['False', 'False']
        expected_response[options.index(correct)] = 'True'

        return {
            "input": inputs,
            "label": expected_response 
        }

# -----------------------------
# GQA-spatial Dataset
# -----------------------------
class GQASpatialDataset(Dataset):
    """
    GQA-spatial Dataset
    """
    def __init__(self, dataset_name="one", data_path="data", image_path="data", model=None, config=None):

        # Validations
        self.base_path = Path(data_path) / "raw" / "GQA_spatial" #relative path
        assert self.base_path.exists(), f"Root directory '{self.base_path}' does not exist."   
        assert dataset_name in ['one', 'two'], f"Unsupported subset: '{dataset_name}'. Must be one of ['one', 'two']."
        
        # Get train/dev/test
        self.dataset_name = dataset_name
        self.model = model

        # Load dataset
        self.data_path = self.base_path / f"vg_qa_{dataset_name}_obj.json"
        self.image_path = image_path
        self.dataset = self._load_json()

        # Input processing
        self.transform = config["transform"]
        self.tokenizer = config["tokenizer"]
        self.processor = config["processor"]
        self.params = config.get("params", {})
        self.score = "mc"

    def _load_json(self):
        with open(self.data_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _load_image(self, image):
        img_path = Path(self.image_path) / f"{image}.jpg"
        if not os.path.exists(img_path):
            raise FileNotFoundError(f"Image not found: {img_path}")
        return Image.open(img_path).convert("RGB")

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        if self.model in ["clip", "siglip", "siglip2", "pecore"]:
            return self._dual_encoder_item(item)
        
        elif self.model in ["qwen2"]:
            return self._qwen_item(item)
            
        else:
            raise NotImplementedError()
    
    # --- ITEM METHODS ---
    def _dual_encoder_item(self, item):
        """Prepare item for Dual Encoder models."""  
        return {
            "image": self._load_image(item[0]),
            "caption_options": [str(item[1]), str(item[2])],
            "correct_option": str(item[1]), # The first option is the correct one
        }
    
    def _qwen_item(self, item):
        """Prepare item for Qwen2-VL model."""
        img_path = Path(self.image_path) / f"{item[0]}.jpg"
        correct = str(item[1])
        options = [str(item[1]), str(item[2])]
        random.shuffle(options)

        # messages = create_MC_qwen_message(img_path, options)
        # text = self.processor.apply_chat_template(
        #     messages, tokenize=False, add_generation_prompt=True
        # )
        # image_inputs, video_inputs = process_vision_info(messages)
        # inputs = self.processor(
        #     text=[text],
        #     images=image_inputs,
        #     videos=video_inputs,
        #     padding=True,
        #     return_tensors="pt",
        # )
        # # Expected response to be generated by Qwen (ej: "A" or "B")
        # label_opt = ['A', 'B']
        # expected_response = label_opt[options.index(correct)]

        inputs = []
        messages = create_qwen_message(img_path, options)
        for message in messages: #Process image - message in pairs
            text = self.processor.apply_chat_template(
                message, tokenize=False, add_generation_prompt=True
            )
            image_inputs, video_inputs = process_vision_info(message)
            input = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            inputs.append(input)

        # Expected response to be generated by Qwen (ej: "True" or "False" for each)
        expected_response = ['False', 'False']
        expected_response[options.index(correct)] = 'True'

        return {
            "input": inputs,
            "label": expected_response 
        }

# -----------------------------
# DATAMODULE
# -----------------------------
class WhatsUpDataModule(pl.LightningDataModule):
    """
    What's Up Data Module
    """
    def __init__(self, args, config): 
        super().__init__()

        self.root = args.root
        self.batch_size = args.batch_size
        self.num_workers = args.num_workers
        self.dataset_name = args.variant 
        self.image_path = args.image_path
        self.dataset = args.dataset
        self.score = args.score
        self.model = args.model
        self.config = config

        # Setup dataloader
        self.setup()

    def setup(self, stage=None):
        """
        Called once at the beginning of training, to prepare datasets.
        """
        # Define collate function (for evaluation)
        if self.model in ["clip", "siglip", "siglip2", "pecore"]: # Dual Encoders
            self.collate_fn_eval = lambda batch: whastup_dual_encoder_collate(
                batch, self.config, self.model # Pasar args y model_name
            )
        else:  #qwen
            self.collate_fn_eval = None

        if self.dataset == "whatsup":
            self.dataset = WhatsUpDataset(
                data_path=self.root,
                dataset_name=self.dataset_name,
                model=self.model,
                config=self.config  
            )

        elif self.dataset == "cocospatial":
            self.dataset = COCOSpatialDataset(
                data_path=self.root,
                image_path=self.image_path,
                dataset_name=self.dataset_name,
                model=self.model,
                config=self.config 
            )

        elif self.dataset == "gqaspatial":
            self.dataset = GQASpatialDataset(
                data_path=self.root,
                image_path=self.image_path,
                dataset_name=self.dataset_name,
                model=self.model,
                config=self.config 
            )
        else: 
            raise NotImplementedError

    def test_dataloader(self):
        return DataLoader(
            self.dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn_eval
        )